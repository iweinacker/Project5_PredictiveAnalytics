{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# Data Cleaning Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "*   Evaluate missing data\n",
    "*   Clean data\n",
    "\n",
    "## Inputs\n",
    "\n",
    "* outputs/datasets/collection/HousePrediction.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "* Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n",
    "\n",
    "## Conclusions\n",
    "\n",
    " \n",
    "  * Data Cleaning Pipeline\n",
    "  * Drop Variables:  `['EnclosedPorch', 'WoodDeckSF' ]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kspgyffxear-"
   },
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* os.path.dirname() gets the parent directory\n",
    "* os.chir() defines the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tdAGw4Zwssu"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load Collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C2ELZj83tF1g",
    "outputId": "d51e2567-7f6c-4200-b9c2-d4982d22e6fd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_raw_path = \"outputs/datasets/collection/HousePrediction.csv\"\n",
    "df = pd.read_csv(df_raw_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iue5e5GJ_vZg"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhWelcb_17dw"
   },
   "source": [
    "In Data Cleaning you are interested to check the distribution and shape of a variable with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyi3gi2-_q1j"
   },
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvwabO0JsmYW"
   },
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6Zy_MglsmYo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=np.bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\")\n",
    "    df_corr_pearson = df.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(df)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ryNo1VnXSK9K"
   },
   "source": [
    "Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_Z4SXf6GbED",
    "outputId": "6978dbde-ebb0-4023-af6c-b6f896ced798"
   },
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJ-0L4PiSPEK"
   },
   "source": [
    "Display at Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ioE3yuC4Q7QK",
    "outputId": "1946ffa1-2e5d-4972-eeda-45d7fe382314"
   },
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
    "                  df_corr_spearman = df_corr_spearman, \n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.4, PPS_Threshold =0.2,\n",
    "                  figsize=(12,10), font_annot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYdZHyDhu4kG"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxoVRefhu2Bk"
   },
   "source": [
    "## Assessing Missing Data Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcocWZkIx6nk"
   },
   "source": [
    "* Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9CoLqBhO7ga"
   },
   "outputs": [],
   "source": [
    "def EvaluateMissingData(df):\n",
    "    missing_data_absolute = df.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(df)*100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHrzaG-ZhEKt"
   },
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "zxvNnLAjxCSi",
    "outputId": "e6239998-21e1-4401-80de-e25c33bbba52"
   },
   "outputs": [],
   "source": [
    "EvaluateMissingData(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWAI2EAp2FNM"
   },
   "source": [
    "## Data Cleaning Spreadsheet Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pp-lSdVH-_-3"
   },
   "source": [
    "* Consider your spreadsheet notes on potential approaches to handle missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Encode object variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_enc = {'BsmtExposure': {'None': 0, 'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}, 'BsmtFinType1': {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}, \n",
    "           'GarageFinish': {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}, 'KitchenQual': {'Po': 0, 'Fa': 1, 'TA': 2, 'Gd': 3, 'Ex': 4}}\n",
    "df_en=df.copy()\n",
    "for col in df.columns[df.dtypes=='object'].to_list():\n",
    "    df_en[col] = df_en[col].replace(dic[col])\n",
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# create a Class variable with a fit and transform method\n",
    "class MyCustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self, variables, dic):\n",
    "    if not isinstance(variables, list): \n",
    "      self.variables = [variables]\n",
    "    else: self.variables = variables\n",
    "    self.dic = dic\n",
    "\n",
    "  def fit(self, X, y=None):    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    for col in self.variables:\n",
    "      if X[col].dtype == 'object':\n",
    "        X[col] = X[col].replace(dic[col])\n",
    "      else:\n",
    "        print(f\"Warning: {col} data type should be object to use MyCustomEncoder()\")\n",
    "      \n",
    "    return X\n",
    "\n",
    "# use the custom encoder in a pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('custom_encoder', MyCustomEncoder(variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'], dic=dic))])\n",
    "\n",
    "df_en = df.copy()\n",
    "df_en = pipeline.fit_transform(df2)\n",
    "df_en.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsL8yYQEyOSt"
   },
   "source": [
    "## Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt8Yqjy6ghyw"
   },
   "source": [
    "### Data Cleaning Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the missing data, the graphs does not appear for the feature_engine.encoding, I decided to do the following steps:\n",
    "* first drop EnclosedPorch and WoodDeckSF as the Missing Data percentage is 90.7% and 89.4% respectively.\n",
    "* The rest, as they not exceed over the 12%\n",
    "    * I decided to fill with the **mean** those data that were numerical.\n",
    "    * an in those that were object,fill it with **missingdata**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W12Z8KIPoZ-8"
   },
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Knk7DcVborLI",
    "outputId": "049ef5f2-c431-4a9d-fd87-00809986c38d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "TrainSet, TestSet, _, __ = train_test_split(\n",
    "                                        df,\n",
    "                                        df['SalePrice'],\n",
    "                                        test_size=0.2,\n",
    "                                        random_state=0)\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape} \\nTestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "id": "zdMPKBU_yCpF",
    "outputId": "df45adaf-5e93-4644-919f-6b82e51fe8b7"
   },
   "outputs": [],
   "source": [
    "df_missing_data = EvaluateMissingData(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUeGUUmu4qru"
   },
   "source": [
    "### Drop Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zlQBeLo44qr8",
    "outputId": "6a342431-6be3-48fa-ce83-ed00489d960c"
   },
   "outputs": [],
   "source": [
    "variables_method = ['WoodDeckSF', 'EnclosedPorch' ]\n",
    "\n",
    "print(f\"* {len(variables_method)} variables to drop \\n\\n\"\n",
    "    f\"{variables_method}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1kKajldGrqF"
   },
   "source": [
    "* We are dropping `WoodDeckSF` and `EnclosedPorch` since they both exceed 80% of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "\n",
    "imputer.fit(TrainSet)\n",
    "df_method = imputer.transform(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "imputer = DropFeatures(features_to_drop=variables_method)\n",
    "imputer.fit(TrainSet)\n",
    "\n",
    "TrainSet, TestSet = imputer.transform(TrainSet) , imputer.transfor+(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "numvariables_method = ['LotFrontage', 'BedroomAbvGr', '2ndFlrSF',\n",
    "                                           'GarageYrBlt', 'MasVnrArea', 'OverallQual',\n",
    "                                           'GrLivArea']\n",
    "\n",
    "numimputer = MeanMedianImputer(imputation_method='median', variables=numvariables_method)\n",
    "\n",
    "numimputer.fit(TrainSet)\n",
    "df_method = numimputer.transform(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "numvariables_method = ['LotFrontage', 'BedroomAbvGr', '2ndFlrSF',\n",
    "                                           'GarageYrBlt', 'MasVnrArea', 'OverallQual',\n",
    "                                           'GrLivArea']\n",
    "\n",
    "numimputer = MeanMedianImputer(imputation_method='median', variables=numvariables_method)\n",
    "\n",
    "numimputer.fit(TrainSet)\n",
    "TrainSet, TestSet = numimputer.transform(TrainSet) , numimputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "catvariables_method = ['GarageFinish', 'BsmtFinType1']\n",
    "\n",
    "catimputer = CategoricalImputer(imputation_method='missing',\n",
    "                            variables=catvariables_method)\n",
    "catimputer.fit(TrainSet)\n",
    "df_method = catimputer.transform(TrainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "catvariables_method = ['GarageFinish', 'BsmtFinType1']\n",
    "\n",
    "catimputer = CategoricalImputer(imputation_method='missing',\n",
    "                            variables=catvariables_method)\n",
    "catimputer.fit(TrainSet)\n",
    "TrainSet, TestSet = catimputer.transform(TrainSet) , catimputer.transform(TestSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7au_UdA34qr9"
   },
   "source": [
    "* Step 3: Create a separate DataFrame applying this imputation approach to the selected variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsjRXMlU4qr9"
   },
   "source": [
    "* Step 4: Assess the effect on the variable's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zc9pg4ww_BO-"
   },
   "source": [
    "* In this case, there is no effect on the distribution of the variable, since you are not removing rows, but columns.\n",
    "* The effect might be losing features that might have a relevant impact on your machine-learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBq1a_Me4qr-"
   },
   "source": [
    "* Step 5: If you are satisfied, apply the transformation to your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LozxNCVO4qr_"
   },
   "source": [
    "* Step 6: Evaluate if you have more variables to deal with. If yes, iterate. If not, you are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "zGmZy46L4qr_",
    "outputId": "3597364f-19a3-42af-8d56-3bbcfe8675e4"
   },
   "outputs": [],
   "source": [
    "EvaluateMissingData(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD--GI5Jvp8h"
   },
   "source": [
    "# Push cleaned data to Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SNWONrYwu6d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "  os.makedirs(name='outputs/datasets/cleaned') # create outputs/datasets/collection folder\n",
    "except Exception as e:\n",
    "  print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE3DQfy82r7I"
   },
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1ayKT4F2p52"
   },
   "outputs": [],
   "source": [
    "TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BTTabVk-2ulD"
   },
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCjorYny2qCD"
   },
   "outputs": [],
   "source": [
    "TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good job! Clear cell outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgNVNpuV6NV2"
   },
   "source": [
    "Well done! You can now push the changes to your GitHub Repo, using the Git commands (git add, git commit, git push)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
